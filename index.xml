<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Home on Hugo Ivy</title>
    <link>https://kkarthik23.github.io/</link>
    <description>Recent content in Home on Hugo Ivy</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 16 Jun 2019 21:57:42 +0000</lastBuildDate>
    
	<atom:link href="https://kkarthik23.github.io/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Packer</title>
      <link>https://kkarthik23.github.io/post/2019/06/16/packer/</link>
      <pubDate>Sun, 16 Jun 2019 21:57:42 +0000</pubDate>
      
      <guid>https://kkarthik23.github.io/post/2019/06/16/packer/</guid>
      <description>Packer is from HashiCorp and is another provisioning tool
a packer folder in where packer.exe resides on Windows will contain
1.variables.json 2.template.json 3.provision.sh  variables.json will contain access key info
{ &amp;quot;aws_access_key&amp;quot;: &amp;quot;&amp;quot;, &amp;quot;aws_secret_key&amp;quot;: &amp;quot;&amp;quot; }  We are using a debian 9 stretch ami and provisioning it
template.json
{ &amp;quot;variables&amp;quot;: { &amp;quot;aws_access_key&amp;quot;: &amp;quot;&amp;quot;, &amp;quot;aws_secret_key&amp;quot;: &amp;quot;&amp;quot; }, &amp;quot;builders&amp;quot;: [{ &amp;quot;type&amp;quot;: &amp;quot;amazon-ebs&amp;quot;, &amp;quot;access_key&amp;quot;: &amp;quot;{{user `aws_access_key`}}&amp;quot;, &amp;quot;secret_key&amp;quot;: &amp;quot;{{user `aws_secret_key`}}&amp;quot;, &amp;quot;region&amp;quot;: &amp;quot;us-east-1&amp;quot;, &amp;quot;source_ami&amp;quot;: &amp;quot;ami-003f19e0e687de1cd&amp;quot;, &amp;quot;instance_type&amp;quot;: &amp;quot;t2.</description>
    </item>
    
    <item>
      <title>Terraform Part 1</title>
      <link>https://kkarthik23.github.io/post/2019/06/10/terraform-part-1/</link>
      <pubDate>Mon, 10 Jun 2019 21:57:42 +0000</pubDate>
      
      <guid>https://kkarthik23.github.io/post/2019/06/10/terraform-part-1/</guid>
      <description>Terraform is an infrastucture orcherstation tool Download from HashiCorp some of the commands are
terraform init terraform plan terraform apply terraform destroy  example.tf file which sets up an instance in a vpc and a subnet in a region after access key and secret key are provided
 provider &amp;quot;aws&amp;quot; { access_key = &amp;quot;&amp;quot; secret_key = &amp;quot;&amp;quot; region = &amp;quot;us-east-1&amp;quot; } resource &amp;quot;aws_instance&amp;quot; &amp;quot;example&amp;quot; { ami = &amp;quot;ami-0c6b1d09930fac512&amp;quot; instance_type = &amp;quot;t2.</description>
    </item>
    
    <item>
      <title>AWS Athena</title>
      <link>https://kkarthik23.github.io/post/2019/05/30/aws-athena/</link>
      <pubDate>Thu, 30 May 2019 21:57:42 +0000</pubDate>
      
      <guid>https://kkarthik23.github.io/post/2019/05/30/aws-athena/</guid>
      <description> Athena for example abl_logs is the database and table is haproxy_tls_inbound_dev
SELECT * FROM &amp;quot;alb_logs&amp;quot;.&amp;quot;haproxy_tls_inbound_dev&amp;quot; limit 10;  </description>
    </item>
    
    <item>
      <title>Python setup windows</title>
      <link>https://kkarthik23.github.io/post/2019/05/20/python-setup-windows/</link>
      <pubDate>Mon, 20 May 2019 21:57:42 +0000</pubDate>
      
      <guid>https://kkarthik23.github.io/post/2019/05/20/python-setup-windows/</guid>
      <description>For Windows, virtualenv venv creates a virtual environment However, activating the environment requires a slightly different command.
Assuming that you are in your project directory:
C:\Users\SomeUser\project_folder&amp;gt; venv\Scripts\activate or source venv/bin/activate (if Linux)
Install packages using the pip command:
$ pip install requests $ deactivate
for reactivating it is venv\Scripts\activate
AWS app deploy using Boto Before you can deploy an application, be sure you have credentials configured. If you have previously configured your machine to run boto3 (the AWS SDK for Python) or the AWS CLI then you can skip this section.</description>
    </item>
    
    <item>
      <title>Prometheus using Docker and WSL</title>
      <link>https://kkarthik23.github.io/post/2019/05/03/prometheus-using-docker-and-wsl/</link>
      <pubDate>Fri, 03 May 2019 11:20:00 +0000</pubDate>
      
      <guid>https://kkarthik23.github.io/post/2019/05/03/prometheus-using-docker-and-wsl/</guid>
      <description>Docker on Windows using WSL Some useful docker commands docker logs containerid docker system prune docker rm containerid docker rmi -f imagid  issue with not reading config file on host chmod 777 directory chmod 777 file  /etc/wsl.conf
[automount] root = / enabled = true options = &amp;quot;metadata&amp;quot;  Make sure c drive is mounted
ls -la /c pwd sudo mkdir prometheus sudo mkdir prometheus_data sudo nano docker-compose.yml  in below docker compose file in volume section what it means is local folder on host is pointing to location on container version: &#39;3&#39; networks: monitor-net: driver: bridge volumes: prometheus_data: grafana_data: services: prometheus: image: prom/prometheus:latest container_name: prometheus volumes: - .</description>
    </item>
    
    <item>
      <title>Source Tree and GitHubDesktop</title>
      <link>https://kkarthik23.github.io/post/2019/05/01/source-tree-and-githubdesktop/</link>
      <pubDate>Wed, 01 May 2019 11:20:00 +0000</pubDate>
      
      <guid>https://kkarthik23.github.io/post/2019/05/01/source-tree-and-githubdesktop/</guid>
      <description>Install SourceTree set up account details of bitbucket
Then do a refresh using oAuth and it will authenticate in browser after this u can clone from remote repository
Set up authentication to bitbucket account Clone repository (Clone is similar to a checkout)
Basic steps
1. Clone remote repository 2. Pull 3. Make changes to files 4. Push  For GitHubDesktop Goto File,Clone Repository,URL
https://username3@bitbucket.org/username/test.git  Basic steps
1. Clone remote repository 2.</description>
    </item>
    
    <item>
      <title>Docker</title>
      <link>https://kkarthik23.github.io/post/2019/04/20/docker/</link>
      <pubDate>Sat, 20 Apr 2019 21:57:42 +0000</pubDate>
      
      <guid>https://kkarthik23.github.io/post/2019/04/20/docker/</guid>
      <description>Docker containers
Install Docker CE(Community Edition) $ sudo yum install -y yum-utils \ device-mapper-persistent-data \ lvm2$ sudo yum-config-manager \ $ sudo yum-config-manager \ --add-repo \ https://download.docker.com/linux/centos/docker-ce.repo $ sudo yum install docker-ce docker-ce-cli containerd.io $ sudo systemctl start docker $ sudo systemctl enable docker (start at boot) $ sudo docker run hello-world (pulls image from docker libaray and indicates docker is working okay)  Above command displays the below
Unable to find image &#39;hello-world:latest&#39; locally latest: Pulling from library/hello-world 1b930d010525: Pull complete Digest: sha256:92695bc579f31df7a63da6922075d0666e565ceccad16b59c3374d2cf4e8e50e Status: Downloaded newer image for hello-world:latest Hello from Docker!</description>
    </item>
    
    <item>
      <title>Open Distro Elastic Search</title>
      <link>https://kkarthik23.github.io/post/2019/04/11/open-distro-elastic-search/</link>
      <pubDate>Thu, 11 Apr 2019 15:20:00 +0000</pubDate>
      
      <guid>https://kkarthik23.github.io/post/2019/04/11/open-distro-elastic-search/</guid>
      <description>After installing Open Distro Elastic Search (check the Open Distro website for instructions) test using
curl -k -X GET &#39;https://localhost:9200&#39; -u username:password  journalctl -u kibana.service this will show recent 50 lines journalctl -u kibana.service -n 50  Filebeat YML #=========================== Filebeat inputs ============================= filebeat.inputs: # Each - is an input. Most options can be set at the input level, so # you can use different inputs for various configurations.</description>
    </item>
    
    <item>
      <title>CentOS Basics</title>
      <link>https://kkarthik23.github.io/post/2019/03/28/centos-basics/</link>
      <pubDate>Thu, 28 Mar 2019 21:57:42 +0000</pubDate>
      
      <guid>https://kkarthik23.github.io/post/2019/03/28/centos-basics/</guid>
      <description>Linux CentOS Basics Centos no ip address
sudo vim /etc/sysconfig/network-scripts/ifcfg-emp0s3
And set the ONBOOT to yes
ONBOOT=yes
Sudo Sestatus Sudo setenforce 0 sudo nano /etc/selinux/config Change SELINUX=enforcing to disabled Sudo reboot
disable firewall systemctl stop firewalld systemctl disable firewalld  VirtualBox NAT port forwarding You can use the address of the host, e.g. 127.0.0.1 if you are on that machine. You will need to set up port forwarding in VirtualBox, which can be found in the networking settings:</description>
    </item>
    
    <item>
      <title>Selenium</title>
      <link>https://kkarthik23.github.io/post/2018/09/05/selenium/</link>
      <pubDate>Wed, 05 Sep 2018 21:57:42 +0000</pubDate>
      
      <guid>https://kkarthik23.github.io/post/2018/09/05/selenium/</guid>
      <description>Now you&amp;rsquo;re ready to write your Selenium script:
 var assert = require(&#39;chai&#39;).assert; // Script-wide timeout for all wait and waitAndFind functions (in ms) var DEFAULT_ELEMENT_TIMEOUT = 190000; //3 mins var DEFAULT_PAGELOAD_TIMEOUT = 240000; //4 mins var navLinks = [&amp;quot;css-locator-1&amp;quot;,&amp;quot;css-locator-2&amp;quot;]; //sets element load timeout to 3 mins $browser.manage().timeouts().implicitlyWait(DEFAULT_ELEMENT_TIMEOUT); //sets page load timoeout to 4 mins $browser.manage().timeouts().pageLoadTimeout(DEFAULT_PAGELOAD_TIMEOUT); //Test all the main Nav page performances $browser.get(&amp;quot;http://www.sitename.com&amp;quot;).then(function(){ return $browser.findElement($driver.By.className(&amp;quot;site-theme-example&amp;quot;)); }).then(function(){ //Verifies the nav list has loaded return $browser.</description>
    </item>
    
    <item>
      <title>Kibana</title>
      <link>https://kkarthik23.github.io/post/2018/07/28/kibana/</link>
      <pubDate>Sat, 28 Jul 2018 21:57:42 +0000</pubDate>
      
      <guid>https://kkarthik23.github.io/post/2018/07/28/kibana/</guid>
      <description>Install Java sudo yum -y install java Run the following command to import the Elasticsearch public GPG key into rpm:
sudo rpm &amp;ndash;import http://packages.elastic.co/GPG-KEY-elasticsearch
Create a new yum repository file for Elasticsearch. Note that this is a single command:
echo &amp;lsquo;[elasticsearch-2.x] name=Elasticsearch repository for 2.x packages baseurl=http://packages.elastic.co/elasticsearch/2.x/centos gpgcheck=1 gpgkey=http://packages.elastic.co/GPG-KEY-elasticsearch enabled=1 &amp;lsquo; | sudo tee /etc/yum.repos.d/elasticsearch.repo
Install Elasticsearch with this command: sudo yum -y install elasticsearch
Elasticsearch is now installed. Let&amp;rsquo;s edit the configuration:</description>
    </item>
    
    <item>
      <title>Vagrant Hyper V</title>
      <link>https://kkarthik23.github.io/post/2018/06/29/vagrant-hyper-v/</link>
      <pubDate>Fri, 29 Jun 2018 21:57:42 +0000</pubDate>
      
      <guid>https://kkarthik23.github.io/post/2018/06/29/vagrant-hyper-v/</guid>
      <description>vagrant box add hashicorp/precise64 &amp;ndash;provider hyperv vagrant up &amp;ndash;provider hyperv After running vagrant init, modify your vagrant file with the following:
Vagrant.configure(2) do |config|
config.vm.box = &amp;ldquo;hashicorp/precise64&amp;rdquo; config.vm.provider &amp;ldquo;hyperv&amp;rdquo; config.vm.network &amp;ldquo;public_network&amp;rdquo; end</description>
    </item>
    
    <item>
      <title>Zabbix Part 2</title>
      <link>https://kkarthik23.github.io/post/2018/06/18/zabbix-part-2/</link>
      <pubDate>Mon, 18 Jun 2018 21:57:42 +0000</pubDate>
      
      <guid>https://kkarthik23.github.io/post/2018/06/18/zabbix-part-2/</guid>
      <description>Zabbix net.tcp.service[ssh,,155]
0 is down 1 is up
create new item and add to template create new trigger
TriggerName sshcheck on {HOST.NAME}
expression
{Template OS Linux:net.tcp.service[ssh,,22].last()}=0</description>
    </item>
    
    <item>
      <title>Powershell snippets Part 2</title>
      <link>https://kkarthik23.github.io/post/2018/06/02/powershell-snippets-part-2/</link>
      <pubDate>Sat, 02 Jun 2018 21:57:42 +0000</pubDate>
      
      <guid>https://kkarthik23.github.io/post/2018/06/02/powershell-snippets-part-2/</guid>
      <description>Powershell snippets Move files for archiving condition  if(Test-Path C:\Pshell) { $Source=&amp;quot;C:\Pshell\*.json&amp;quot; $Destination=&amp;quot;C:\Pshell\json\&amp;quot; Move-Item $Source $Destination } { }  New Relic API sample &amp;lt;# $apikey = &#39;abc&#39; $Appname = &#39;prod-publicwebsite-aws&#39; function Get-NewRelicAppId { Param ( [string]$NRAppName, [string]$NRApiKey ) Write-Debug -Message &amp;quot;here&amp;quot; $URL = &amp;quot;https://api.newrelic.com/v2/applications.json&amp;quot; $URI = New-Object System.Uri($URL,$true) $WebRequestData = @{ Uri = $URI Headers = @{&amp;quot;X-Api-Key&amp;quot;=&amp;quot;$NRApiKey&amp;quot;} ContentType = &#39;application/json&#39; Body = @{&amp;quot;filter[name]&amp;quot;=&amp;quot;$NRAppName&amp;quot;} } $Results = Invoke-RestMethod -Method GET @WebRequestData $NRAppData = $Results.</description>
    </item>
    
    <item>
      <title>Django</title>
      <link>https://kkarthik23.github.io/post/2018/05/28/django/</link>
      <pubDate>Mon, 28 May 2018 21:57:42 +0000</pubDate>
      
      <guid>https://kkarthik23.github.io/post/2018/05/28/django/</guid>
      <description>Django Basics Creating a new Django Project
To create the project run,
python django-admin.py startproject 
the name of the project you wish to create.
Creating a new Django App
 To create a new app, run 
$ python manage.py startapp &amp;lt;appname&amp;gt;  name of the app you wish to create.
 Tell your Django project about the new app by adding it to the tuple in your INSTALLED_APPS project’s file settings.</description>
    </item>
    
    <item>
      <title>Xymon and Xymon Client</title>
      <link>https://kkarthik23.github.io/post/2018/05/28/xymon-and-xymon-client/</link>
      <pubDate>Mon, 28 May 2018 21:57:42 +0000</pubDate>
      
      <guid>https://kkarthik23.github.io/post/2018/05/28/xymon-and-xymon-client/</guid>
      <description>Install xymon and apache2 on Debian Stretch
sudo apt-get install apache2 sudo apt-get install xymon  Copy the ‘xymon’ file located in /etc/apache2/conf.d to /etc/apache2/conf-available –but– change it from xymon to xymon.conf (command: sudo cp /etc/apache2/conf.d/xymon /etc/apache2/conf-available/xymon.conf)
Now, create a symlink to that file in the conf-enabled folder (command: sudo ln -s /etc/apache2/conf-available/xymon.conf /etc/apache2/conf-enabled/ )
Edit the xymon.conf file in /etc/apache2/conf-available and remove any lines with “Order allow,deny” and “All from localhost…” and put “Require all granted” in their place</description>
    </item>
    
    <item>
      <title>Ansible Part 1</title>
      <link>https://kkarthik23.github.io/post/2018/05/20/ansible-part-1/</link>
      <pubDate>Sun, 20 May 2018 21:57:42 +0000</pubDate>
      
      <guid>https://kkarthik23.github.io/post/2018/05/20/ansible-part-1/</guid>
      <description>Set up an AWS instance
Download the ssh key for the user which is a pem key then use Putty Gen to import the pem key and save it as a ppk key Then use ec2-user@instancename on port 22 and save session In auth filed use the ppk key to connect
EPEL Release epel-release is available in Amazon Linux Extra topic &amp;ldquo;epel&amp;rdquo;
To use, run sudo amazon-linux-extras install epel</description>
    </item>
    
    <item>
      <title>Collectd and Graphite</title>
      <link>https://kkarthik23.github.io/post/2018/05/18/collectd-and-graphite/</link>
      <pubDate>Fri, 18 May 2018 21:57:42 +0000</pubDate>
      
      <guid>https://kkarthik23.github.io/post/2018/05/18/collectd-and-graphite/</guid>
      <description> Install Collectd sudo yum install collectd  A minor configuration change will be needed to direct Collectd to report metrics to your Graphite server. Edit the configuration file (typically found at /etc/collectd/collectd.conf) and then restart the collectd service.
LoadPlugin write_graphite &amp;lt;Plugin write_graphite&amp;gt; &amp;lt;Node &amp;quot;example&amp;quot;&amp;gt; Host &amp;quot;localhost&amp;quot; Port &amp;quot;2003&amp;quot; Prefix &amp;quot;collectd&amp;quot; &amp;lt;/Node&amp;gt; &amp;lt;/Plugin&amp;gt;  Check if Graphite and Collectd are running
netstat -nltp | grep &#39;[27]00&#39; sudo systemctl status collectd  </description>
    </item>
    
    <item>
      <title>Grafana</title>
      <link>https://kkarthik23.github.io/post/2018/04/06/grafana/</link>
      <pubDate>Fri, 06 Apr 2018 21:57:42 +0000</pubDate>
      
      <guid>https://kkarthik23.github.io/post/2018/04/06/grafana/</guid>
      <description>Install  Redhat &amp;amp; Centos(64 Bit) wget https://s3-us-west-2.amazonaws.com/grafana-releases/release/grafana-5.1.3-1.x86_64.rpm sudo yum localinstall grafana-5.1.3-1.x86_64.rpm sudo systemctl enable grafana-server.service sudo systemctl start grafana-server  Test using this URL or the corresponding IP address
http://localhost:3000  A number of integrations available for example for integrating with Prometheus,InfluDB,Zabbix etc.</description>
    </item>
    
    <item>
      <title>3DS</title>
      <link>https://kkarthik23.github.io/post/2018/01/20/3ds/</link>
      <pubDate>Sat, 20 Jan 2018 21:57:42 +0000</pubDate>
      
      <guid>https://kkarthik23.github.io/post/2018/01/20/3ds/</guid>
      <description>3DS 1.0 It allows shoppers to assign a password to their card that is then verified whenever a payment is processed through a shop that supports 3D Secure. The addition of password protection allows extra security on transactions that are processed online. The scheme is a collective of Verified by VISA (VBV) and MasterCard Secure Code (MSC). 3D Secure offers companies liability cover for transactions that are verified by checks.</description>
    </item>
    
    <item>
      <title>SQL Related</title>
      <link>https://kkarthik23.github.io/post/2018/01/02/sql-related/</link>
      <pubDate>Tue, 02 Jan 2018 21:57:42 +0000</pubDate>
      
      <guid>https://kkarthik23.github.io/post/2018/01/02/sql-related/</guid>
      <description>Implicit Conversion Implicit Conversion might kill your SQL Query performance so its important that its fixed

In this picture the first query does not use implicit conversion and so the index could be used and performance is better but the second uses implicit conversion and so the SQL engine couldn&amp;rsquo;t use an index and so performance suffers.
SARG SARG is an acronym of “Search ARGumentable”. SARGable is defined as “In relational databases, a condition (or predicate) in a query is said to be sargable if the DBMS engine can take advantage of an index to speed up the execution of the query.</description>
    </item>
    
    <item>
      <title>APM tools</title>
      <link>https://kkarthik23.github.io/post/2017/12/12/apm-tools/</link>
      <pubDate>Tue, 12 Dec 2017 21:57:42 +0000</pubDate>
      
      <guid>https://kkarthik23.github.io/post/2017/12/12/apm-tools/</guid>
      <description>APM Application Performance Tools are crucial in monitoring the performance of applications.
They give a better insight into the behaviour of your applications and are worth it.
New Relic is the most well known one but there are others in the market as well.
Example This the top 5 memory consumers of w3p processes in an IIS web server.
New Relic has a few criteria for mesuring app performance</description>
    </item>
    
    <item>
      <title>Prometheus</title>
      <link>https://kkarthik23.github.io/post/2017/12/11/prometheus/</link>
      <pubDate>Mon, 11 Dec 2017 15:20:00 +0000</pubDate>
      
      <guid>https://kkarthik23.github.io/post/2017/12/11/prometheus/</guid>
      <description>Prometheus Install mkdir Downloads cd Downloads
Download
curl -LO &amp;quot;https://github.com/prometheus/prometheus/releases/download/v2.2.1/prometheus-2.2.1.linux-amd64.tar.gz&amp;quot; goto Root directory mkdir Prometheus cd Prometheus tar -xvzf ~/Downloads/prometheus-2.2.1.linux-amd64.tar.gz ./prometheus --version sudo vi /etc/systemd/system/node_exporter.service  This file should contain the path of the node_exporter executable, and also specify which user should run the executable. Accordingly, add the following code:
/etc/init/node_exporter.conf [Unit] Description=Node Exporter [Service] User=prometheus ExecStart=/home/prometheus/Prometheus/node_exporter/node_exporter [Install] WantedBy=default.target  Reload systemd so that it reads the configuration file you just created.</description>
    </item>
    
    <item>
      <title>Useful Linux Tools</title>
      <link>https://kkarthik23.github.io/post/2017/12/06/useful-linux-tools/</link>
      <pubDate>Wed, 06 Dec 2017 21:57:42 +0000</pubDate>
      
      <guid>https://kkarthik23.github.io/post/2017/12/06/useful-linux-tools/</guid>
      <description>Top is a good tool for viewing system processes
However there is another one with a little bit more information called Htop</description>
    </item>
    
    <item>
      <title>Splunk related</title>
      <link>https://kkarthik23.github.io/post/2017/11/28/splunk-related/</link>
      <pubDate>Tue, 28 Nov 2017 21:57:42 +0000</pubDate>
      
      <guid>https://kkarthik23.github.io/post/2017/11/28/splunk-related/</guid>
      <description>Splunk is a good tool for indexing and searching logs. Splunk uses SPL Splunk Processing language for querying
Splunk common ports are as below
To ignore certain errors so as to prevent from being alerted for false positives index=prodapplications sourcetype=&amp;quot;someservice&amp;quot; level=error message!=&amp;quot;*Error processing&amp;quot; | table dateTime,LEVEL,logger,message,exception  Search the access logs, and return the number of hits from the top 100 values of &amp;ldquo;referer_domain&amp;rdquo;. sourcetype=access_combined | top limit=100 referer_domain | stats sum(count)  Graph the average &amp;ldquo;thruput&amp;rdquo; of hosts over time.</description>
    </item>
    
    <item>
      <title>WebServers</title>
      <link>https://kkarthik23.github.io/post/2017/11/16/webservers/</link>
      <pubDate>Thu, 16 Nov 2017 21:57:42 +0000</pubDate>
      
      <guid>https://kkarthik23.github.io/post/2017/11/16/webservers/</guid>
      <description>Nginx is a web server which is less resource hungry than Apache Nginx Cache Set the Cache Expires field
Refer http://nginx.org/en/docs/http/ngx_http_headers_module.html
Modified /etc/nginx/sites-available/default
map $sent_http_content_type $expires { default off; text/html epoch; text/css max; application/javascript max; ~image/ max; } server { listen 80 default_server; listen [::]:80 default_server; expires $expires; }  Load test Use curl as below and replace with your website
curl -s http://www.google.com?[1-1000]  The other tool is apache benchmark (ab tool) which comes with apache On windows we can use as below provided apache is installed and present in the below location</description>
    </item>
    
    <item>
      <title>Node and Ghost</title>
      <link>https://kkarthik23.github.io/post/2017/11/11/node-and-ghost/</link>
      <pubDate>Sat, 11 Nov 2017 15:20:00 +0000</pubDate>
      
      <guid>https://kkarthik23.github.io/post/2017/11/11/node-and-ghost/</guid>
      <description> Node install Install node JS using windows installer 64 bit edition
Install NVM Windows Node Version Manager Download from https://github.com/coreybutler/nvm-windows Install the windows installer nvm install 8.9.1 nvm version nvm list nvm on  Ghost npm install -g ghost-cli@latest ghost install (for prod) ghost install local (for local) ghost ls ghost stop  </description>
    </item>
    
    <item>
      <title>Notepad&#43;&#43; extension</title>
      <link>https://kkarthik23.github.io/post/2017/11/08/notepad---extension/</link>
      <pubDate>Wed, 08 Nov 2017 20:00:00 +0000</pubDate>
      
      <guid>https://kkarthik23.github.io/post/2017/11/08/notepad---extension/</guid>
      <description>&lt;p&gt;Notepad++ comes with the default theme and it is white background, to change it
Download Material Theme from &lt;a href=&#34;https://github.com/naderi/material-theme-for-npp&#34;&gt;https://github.com/naderi/material-theme-for-npp&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Powershell some commands</title>
      <link>https://kkarthik23.github.io/post/2017/11/05/powershell-some-commands/</link>
      <pubDate>Sun, 05 Nov 2017 14:20:00 +0000</pubDate>
      
      <guid>https://kkarthik23.github.io/post/2017/11/05/powershell-some-commands/</guid>
      <description>Powershell commands Recycle app pool - 2003 web server $appPool = &amp;quot;Test&amp;quot; IIsApp /a &amp;quot;$appPool&amp;quot; /r  Recycle app pool - 2012 web server: $appPool = &amp;quot;Test&amp;quot; Import-Module -Name WebAdministration # Restart app poool Restart-WebAppPool -Name &amp;quot;$appPool&amp;quot;  Get app pool status: Write-Output &amp;quot;App pool $appPool restarted&amp;quot; Write-Output $(Get-WebAppPoolState -Name &amp;quot;$appPool&amp;quot;).Value  Get restart confirmation Start-Sleep -Seconds 30 Get-EventLog -LogName System -Newest 1 -Source WAS -Message &amp;quot;*$appPool*&amp;quot; | Format-List TimeWritten,Message  Restart Windows service - 2012 or 2003: $service = &amp;quot;TestService&amp;quot; Restart-Service -Name &amp;quot;$service&amp;quot; Write-Output &amp;quot;$service restarted&amp;quot; $status = (Get-Service -Name &amp;quot;$service&amp;quot;).</description>
    </item>
    
    <item>
      <title>Cockpit</title>
      <link>https://kkarthik23.github.io/post/2017/10/28/cockpit/</link>
      <pubDate>Sat, 28 Oct 2017 17:20:00 +0000</pubDate>
      
      <guid>https://kkarthik23.github.io/post/2017/10/28/cockpit/</guid>
      <description>&lt;p&gt;Cockpit is a server manager that makes it easy to administer your GNU/Linux servers via a web browser.&lt;/p&gt;

&lt;p&gt;Cockpit works with RedHat,CentOS,Fedora,Ubuntu,Debian&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Set Static Hostname on RHEL/CentOS 7 Linux AWS</title>
      <link>https://kkarthik23.github.io/post/2017/10/14/set-static-hostname-on-rhel/centos-7-linux-aws/</link>
      <pubDate>Sat, 14 Oct 2017 18:20:00 +0000</pubDate>
      
      <guid>https://kkarthik23.github.io/post/2017/10/14/set-static-hostname-on-rhel/centos-7-linux-aws/</guid>
      <description> How to set static hostname on RHEL/CentOS 7 Linux AWS /etc/sysconfig/network HOSTNAME=desired-hostname.example.com  /etc/hostname desired-hostname.example.com  /etc/cloud/cloud.cfg: preserve_hostname: true  </description>
    </item>
    
    <item>
      <title>AWS NLB</title>
      <link>https://kkarthik23.github.io/post/2017/10/14/aws-nlb/</link>
      <pubDate>Sat, 14 Oct 2017 15:20:00 +0000</pubDate>
      
      <guid>https://kkarthik23.github.io/post/2017/10/14/aws-nlb/</guid>
      <description>A couple of my colleagues were fiddling around with NLB (Network Load Balancer) and couldn&amp;rsquo;t find the instance name in target and only IP address of the instances was listed. After some time not being able to find out we logged a support call with AWS and was directed to http://docs.aws.amazon.com/elasticloadbalancing/latest/network/load-balancer-target-groups.html
Limits You cannot register instances by instance ID if they have the following instance types: C1, CC1, CC2, CG1, CG2, CR1, G1, G2, HI1, HS1, M1, M2, M3, and T1.</description>
    </item>
    
    <item>
      <title>EPEL</title>
      <link>https://kkarthik23.github.io/post/2017/10/09/epel/</link>
      <pubDate>Mon, 09 Oct 2017 14:20:00 +0000</pubDate>
      
      <guid>https://kkarthik23.github.io/post/2017/10/09/epel/</guid>
      <description>&lt;p&gt;Some packages like NGINX isn&amp;rsquo;t included in the standard CentOS repositories, you must install an extra package to make sure that, once installed, NGINX is always up to date.&lt;/p&gt;

&lt;p&gt;To do that, we&amp;rsquo;ll install the Extra Packages For Enterprise Linux (EPEL) package.&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Nagios</title>
      <link>https://kkarthik23.github.io/post/2017/10/07/nagios/</link>
      <pubDate>Sat, 07 Oct 2017 19:20:00 +0000</pubDate>
      
      <guid>https://kkarthik23.github.io/post/2017/10/07/nagios/</guid>
      <description>&lt;p&gt;Nagios is a monitoring system&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Xymon</title>
      <link>https://kkarthik23.github.io/post/2017/10/07/xymon/</link>
      <pubDate>Sat, 07 Oct 2017 19:20:00 +0000</pubDate>
      
      <guid>https://kkarthik23.github.io/post/2017/10/07/xymon/</guid>
      <description>&lt;h3 id=&#34;xymon&#34;&gt;Xymon&lt;/h3&gt;

&lt;p&gt;This is a simple but powerful monitoring tool&lt;/p&gt;

&lt;p&gt;Install it from source or using rpms&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Zabbix</title>
      <link>https://kkarthik23.github.io/post/2017/10/07/zabbix/</link>
      <pubDate>Sat, 07 Oct 2017 19:20:00 +0000</pubDate>
      
      <guid>https://kkarthik23.github.io/post/2017/10/07/zabbix/</guid>
      <description>&lt;p&gt;Zabbix is another monitoring system and runs on Linux and uses PHP and MySQL and can run on Apache Web Server&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Static Site generator</title>
      <link>https://kkarthik23.github.io/post/2017/08/26/static-site-generator/</link>
      <pubDate>Sat, 26 Aug 2017 21:57:42 +0000</pubDate>
      
      <guid>https://kkarthik23.github.io/post/2017/08/26/static-site-generator/</guid>
      <description>Hugo is a static Site generator using Go
Some settings needs to be configured to generate output and push to github
In cofig.toml set baseURL = &amp;quot;https://username.github.io/&amp;quot;  run hugo
c:\hugo\quickstart\hugo
files are generated under public folder type c:\hugo\quickstart cd public and then run gitpush.bat which contains below
git init git add . git commit -m &amp;quot;first commit&amp;quot; git remote add origin https://github.com/username/username.github.io git push -u origin master  Hugo is a static generator Some settings needs to be configured to generate output and push to github</description>
    </item>
    
    <item>
      <title>AWS</title>
      <link>https://kkarthik23.github.io/post/2017/08/26/aws/</link>
      <pubDate>Sat, 26 Aug 2017 19:20:00 +0000</pubDate>
      
      <guid>https://kkarthik23.github.io/post/2017/08/26/aws/</guid>
      <description>&lt;p&gt;AWS is Amazon Web Services and is a public cloud&lt;/p&gt;

&lt;p&gt;&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>GIT and Vagrant</title>
      <link>https://kkarthik23.github.io/post/2017/08/06/git-and-vagrant/</link>
      <pubDate>Sun, 06 Aug 2017 21:57:42 +0000</pubDate>
      
      <guid>https://kkarthik23.github.io/post/2017/08/06/git-and-vagrant/</guid>
      <description>GIT GIT is a Distributed Version Control System Some commands are
create a new repository on the command line echo &amp;ldquo;# website&amp;rdquo; &amp;gt;&amp;gt; README.md
git init git add README.md git commit -m &amp;quot;first commit&amp;quot; git remote add origin https://github.com/Username/website.git git push -u origin master  or push an existing repository from the command line git remote add origin https://github.com/Username/website.git git push -u origin master  View remote and remove git remote -v git remote rm destination git remote -v  BitBucket Step 1:Push your local repo cd /path/to/your/repo  Step 2: Connect your existing repository to Bitbucket git remote add origin https://username@bitbucket.</description>
    </item>
    
    <item>
      <title>A Case for Software Load Balancing</title>
      <link>https://kkarthik23.github.io/post/2016/11/12/a-case-for-software-load-balancing/</link>
      <pubDate>Sat, 12 Nov 2016 21:57:42 +0000</pubDate>
      
      <guid>https://kkarthik23.github.io/post/2016/11/12/a-case-for-software-load-balancing/</guid>
      <description>You can run software load balancers on commodity hardware.
Plus virtualisation,containers like docker and tools like puppet and ansible help to leverage software load balancer to the maximum extent.
Unless your requirement is really really big software load balancers should suffice. Nginx does a good job and you can go with Nginx plus for paid support,more features and peace of mind.
There is good documentation on the Nginx website and so I will not be repeating the steps.</description>
    </item>
    
    <item>
      <title>About</title>
      <link>https://kkarthik23.github.io/about/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://kkarthik23.github.io/about/</guid>
      <description>Blog of Karthik Krishna (KK)
KK likes Open Source
Favourite Linux Distros Centos,Fedora,Debian and Ubuntu
Current Interests Containers,CI/CD,Infrastructure as Code</description>
    </item>
    
  </channel>
</rss>